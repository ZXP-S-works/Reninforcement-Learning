{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: In the previous homeworks, we had to deal with a number of complications which made the grading process unnecessarily harder.  While some were a result of us not making explicit requests about programming practices, others were a result of students not following explicit instructions.  Most of these are very easy for each of you to address individually, while they end up piling up on our end.  Before submitting, make sure you are adhering to the following rules, which helps us grade your assignment.  Each rule is associated with a penalty.\n",
    "\n",
    " * (-1pts) Make sure your notebook only contains the exercises requested in the notebook, and the written homework (if any) is delivered in class in printed form, i.e. don't submit your written homework as part of the notebook.\n",
    " * (-1pts) Make sure you are using Python3.  This notebook is already set up to use Python3 (top right corner);  Do not change this.\n",
    " * (-1pts) If a method is provided with a specific signature, do not change the signature in any way, or the default values.\n",
    " * (-1pts) Don't hard-code your solutions to the specific environments which it is being used on, or the specific hyper-parameters which it is being used on;  Be as general as possible, which means also using ALL the arguments of the methods your are implementing.\n",
    " * (-1pts) Make sure your submission file follows the format indicated in blackboard and, most importantly, contains your name.\n",
    " * (-1pts) Clean up your code before submitting, i.e. remove all print statements that you've used to develop and debug (especially if it's going to clog up the interface by printing thousands of lines).  Only output whatever is required by the exercise.\n",
    " * (-2pts) For technical reasons, plots should be contained in their own cell which should run instantly, separate from cells which perform longer computations.  This notebook is already formatted in such a way, please make sure this remains the case.\n",
    " * (-5pts or more) Make sure your notebook runs completely, from start to end, without raising any unintended errors.  After you've made the last edit, Use the option `Kernel -> Restart & Run All` to rerun the entire notebook.  If you end up making ANY edit, re-run everything again.  Always assume any edit you make may have broken your code!  About 10% of the previous homeworks would not run from start to end until something was fixed, which represented a huge effort for the grading process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: TD Learning\n",
    "\n",
    "In this assignment you will implement Sarsa, Expected Sarsa, and Q-Learning and test these algorithms on the Frozen-lake environment and a Cartpole environment with a discrete state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from more_itertools import windowed\n",
    "import tqdm\n",
    "\n",
    "from gym import ObservationWrapper\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = 18, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Utilities\n",
    "\n",
    "Some are provided, some you should implement\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "In this homework, we'll do some plotting of noisy data, so here is the smoothing function which was also used in the previous homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_average(data, *, window_size):\n",
    "    \"\"\"Smoothen the 1-d data array using a rollin average.\n",
    "    \n",
    "    Args:\n",
    "        data: 1-d numpy.array\n",
    "        window_size: size of the smoothing window\n",
    "        \n",
    "    Returns:\n",
    "        smooth_data: a 1-d numpy.array with the same size as data\n",
    "    \"\"\"\n",
    "    assert data.ndim == 1\n",
    "    kernel = np.ones(window_size)\n",
    "    smooth_data = np.convolve(data, kernel) / np.convolve(np.ones_like(data), kernel)\n",
    "    return smooth_data[: -window_size + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-Greedy Action Selection\n",
    "\n",
    "Since you've implemented this method in the previous homework, I'll cut you some slack and provide it for you =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_epsilon_greedy(Q_array, *, eps):\n",
    "    \"\"\"Perform epsilon greedy action selection based on the Q-values.\n",
    "\n",
    "    :param Q_array: A numpy array that contains Q-values relative to a single state.\n",
    "    :param eps: The probability to select a random action. Float between 0 and 1.\n",
    "\n",
    "    Shapes:\n",
    "        Q_array: `(A,)` where `A` is the number of actions.\n",
    "        output: Scalar.\n",
    "    \"\"\"\n",
    "    assert Q_array.ndim == 1\n",
    "\n",
    "    if rnd.rand() < eps:\n",
    "        num_actions = Q_array.shape[0]\n",
    "        action = rnd.randint(num_actions)\n",
    "    else:\n",
    "        actions_maximal = np.argwhere(Q_array == Q_array.max()).flatten()  # all maximal actions\n",
    "        action = rnd.choice(actions_maximal)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-Greedy Decay\n",
    "\n",
    "A fairly typical thing to do when using a $\\epsilon$-greedy exploration strategy is to anneal $\\epsilon$ from its starting value to some final value over a number of timesteps. This allows your agent to explore the environment more at the beginning of training, and less later on.  If the method we are implementing requires an $\\epsilon$-soft policy, then you should not anneal the $\\epsilon$ to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSchedule:\n",
    "    def __init__(self, value_from, value_to, nsteps):\n",
    "        \"\"\"Linear schedule from `value_from` to `value_to` in `nsteps` steps.\n",
    "\n",
    "        :param value_from: initial value\n",
    "        :param value_to: final value\n",
    "        :param nsteps: number of steps for the linear schedule\n",
    "        \"\"\"\n",
    "        self.value_from = value_from\n",
    "        self.value_to = value_to\n",
    "        self.nsteps = nsteps\n",
    "\n",
    "    def value(self, step) -> float:\n",
    "        \"\"\"Return interpolated value between `value_from` and `value_to`.\n",
    "\n",
    "        returns {\n",
    "            `value_from`, if step == 0 or less\n",
    "            `value_to`, if step == nsteps - 1 or more\n",
    "            the interpolation between `value_from` and `value_to`, if 0 <= steps < nsteps\n",
    "        }\n",
    "\n",
    "        :param step:  The step at which to compute the interpolation.\n",
    "        :rtype: float.  The interpolated value.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE (use the tests below if you need examples of what this should produce.)\n",
    "        \n",
    "        return value\n",
    "\n",
    "\n",
    "# test code, do not edit\n",
    "\n",
    "\n",
    "def _test_schedule(schedule, step, value, ndigits=5):\n",
    "    \"\"\"Tests that the schedule returns the correct value.\"\"\"\n",
    "    v = schedule.value(step)\n",
    "    if not round(v, ndigits) == value:\n",
    "        raise Exception(f'For step {step}, the scheduler returned {v} instead of {value}')\n",
    "\n",
    "\n",
    "_schedule = LinearSchedule(0.1, 0.2, 3)\n",
    "_test_schedule(_schedule, -1, 0.1)\n",
    "_test_schedule(_schedule, 0, 0.1)\n",
    "_test_schedule(_schedule, 1, 0.15)\n",
    "_test_schedule(_schedule, 2, 0.2)\n",
    "_test_schedule(_schedule, 3, 0.2)\n",
    "del _schedule\n",
    "\n",
    "_schedule = LinearSchedule(0.5, 0.0, 6)\n",
    "_test_schedule(_schedule, -1, 0.5)\n",
    "_test_schedule(_schedule, 0, 0.5)\n",
    "_test_schedule(_schedule, 1, 0.4)\n",
    "_test_schedule(_schedule, 2, 0.3)\n",
    "_test_schedule(_schedule, 3, 0.2)\n",
    "_test_schedule(_schedule, 4, 0.1)\n",
    "_test_schedule(_schedule, 5, 0.0)\n",
    "_test_schedule(_schedule, 6, 0.0)\n",
    "del _schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete CartPole\n",
    "\n",
    "In this homework we will use a new environment called CartPole (see [here](https://github.com/openai/gym/wiki/CartPole-v0)).  This is a very popular environment, often used as benchmark to make sure that new methods work on simple domains.\n",
    "\n",
    "Compared to the Frozen Lake environment, this one has *continuous* states.  So far we have only addressed the case of discrete states and actions;  as a result, all of the methods we have seen so far have been \"tabular\", i.e. they used dictionaries / hash-maps to store policies and values.  This is not possible for continuous states because, among other reasons, it is often virtually impossible to visit the exact same state twice.  We will formally address the problem of continuous states later in the course, using function approximators.  In the meantime, we will create a gym.Wrapper which discretizes the observed states (a.k.a observations).  Generally, the finer the discretization the more optimal your policy will be but the longer it will take to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteWrapper(ObservationWrapper):\n",
    "    def __init__(self, env, decimals):\n",
    "        super().__init__(env)\n",
    "        self.decimals = decimals\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return tuple(observation.round(self.decimals))\n",
    "\n",
    "\n",
    "frozen_lake = FrozenLakeEnv(map_name='4x4', is_slippery=False)\n",
    "frozen_lake_slippery = FrozenLakeEnv(map_name='4x4', is_slippery=True)\n",
    "cartpole_discrete = DiscreteWrapper(CartPoleEnv(), decimals=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (30 Points):\n",
    "\n",
    "#### a) Implement the on-policy TD control algorithm known as SARSA.  Make sure that the exploration schedule is applied based on how many time-steps have passed from the beginning of the training, not the beginning of the episode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, num_steps_max, *, alpha, gamma, exploration):\n",
    "    \"\"\"SARSA algorithm.\n",
    "\n",
    "    Args:\n",
    "        - env: The Gym environment\n",
    "        - num_episodes: The number of episodes for which to train the agent\n",
    "        - num_steps_max: The maximum number of steps for a single episode\n",
    "        - alpha: The stepsize\n",
    "        - gamma: The discount factor\n",
    "        - exploration: epsilon schedule\n",
    "\n",
    "    Returns: (Q, returns, lengths)\n",
    "        - Q: Dictonary mapping state -> action values\n",
    "        - returns: Numpy array containing the reward of each episode during training\n",
    "        - lengths: Numpy array containing the length of each episode during training\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    returns = np.empty(num_episodes)\n",
    "    lengths = np.empty(num_episodes, dtype=np.int)\n",
    "\n",
    "    for i in tqdm.tnrange(num_episodes, desc='Episodes', leave=False):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    return Q, returns, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Test your implentation of SARSA on the non-slippery FrozenLake environment with $\\gamma=1.0$.  Find the best $\\alpha$ and exploration strategy.  Plot the episode returns and episode lengths over number of training episodes.\n",
    "\n",
    "It might be helpful to also smooth the curves to get a clearer picture of the learning process.  If you do so, plot both the unsmoothened data and the smoothened data in the same figure, and remember to label the axes and the lines.  Make your plots readable **without having to look at your code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint:  you may temporarily change these values to help development, but make sure you set them back before submitting!\n",
    "num_runs, num_episodes, num_steps_max = 50, 1_000, 100\n",
    "returns_sarsa_fl = np.empty((num_runs, num_episodes))\n",
    "lengths_sarsa_fl = np.empty((num_runs, num_episodes), dtype=np.int)\n",
    "\n",
    "for i in tqdm.tnrange(num_runs, desc='Runs'):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [graduate, 5 pts] The non-slippery FrozenLake environment is deterministic, and we are applying no discounting to the rewards ($\\gamma=1.0$);  this means that there is no incentive to exit quickly, and an optimal policy can wander around indefinitely as long as it eventually exits.  However, you should notice in your plots a negative correlation between episode return and episode length, indicating that good performance in your learned policy is somehow associated with shorter episodes.  Explain this apparent contradiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Test your implentation of SARSA on the slippery FrozenLake environment with $\\gamma=1.0$.  Find the best $\\alpha$ and exploration strategy.  Plot the episode returns and episode lengths over number of training episodes.\n",
    "\n",
    "It might be helpful to also smooth the curves to get a clearer picture of the learning process.  If you do so, plot both the unsmoothened data and the smoothened data in the same figure, and remember to label the axes and the lines.  Make your plots readable **without having to look at your code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint:  you may temporarily change these values to help development, but make sure you set them back before submitting!\n",
    "num_runs, num_episodes, num_steps_max = 10, 20_000, 100\n",
    "returns_sarsa_fls = np.empty((num_runs, num_episodes))\n",
    "lengths_sarsa_fls = np.empty((num_runs, num_episodes), dtype=np.int)\n",
    "\n",
    "for i in tqdm.tnrange(num_runs, desc='Runs'):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [graduate, 5 pts] In the non-slippery FrozenLake environment, you should notice a negative correlation between return and episode length.  In the slippery FrozenLake environment, you should notice a positive correlation between return and episode length.  Why does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Test your implentation of SARSA on the discrete CartPole environment with $\\gamma=1.0$.  Find the best $\\alpha$ and exploration strategy.  Plot the episode returns and episode lengths over number of training episodes.\n",
    "\n",
    "It might be helpful to also smooth the curves to get a clearer picture of the learning process.  If you do so, plot both the unsmoothened data and the smoothened data in the same figure, and remember to label the axes and the lines.  Make your plots readable **without having to look at your code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hint:  you may temporarily change these values to help development, but make sure you set them back before submitting!\n",
    "num_runs, num_episodes, num_steps_max = 10, 30_000, 100\n",
    "returns_sarsa_cpd = np.empty((num_runs, num_episodes))\n",
    "lengths_sarsa_cpd = np.empty((num_runs, num_episodes), dtype=np.int)\n",
    "\n",
    "for i in tqdm.tnrange(num_runs, desc='Runs'):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (20 Points):\n",
    "\n",
    "#### a) Implement the on-policy TD control algorithm known as Expected-SARSA. Make sure that the exploration schedule is applied based on how many time-steps have passed from the beginning of the training, not the beginning of the episode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(\n",
    "    env, num_episodes, num_steps_max, *, alpha, gamma, exploration\n",
    "):\n",
    "    \"\"\"Expected SARSA algorithm.\n",
    "\n",
    "    Args:\n",
    "        - env: The Gym environment\n",
    "        - num_episodes: The number of episodes for which to train the agent\n",
    "        - num_steps_max: The maximum number of steps for a single episode\n",
    "        - alpha: The stepsize\n",
    "        - gamma: The discount factor\n",
    "        - exploration: epsilon schedule\n",
    "\n",
    "    Returns: (Q, returns, lengths)\n",
    "        - Q: Dictonary mapping state -> action values\n",
    "        - returns: Numpy array containing the reward of each episode during training\n",
    "        - lengths: Numpy array containing the length of each episode during training\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    returns = np.empty(num_episodes)\n",
    "    lengths = np.empty(num_episodes, dtype=np.int)\n",
    "\n",
    "    for i in tqdm.tnrange(num_episodes, desc='Episodes', leave=False):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    return Q, returns, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Test your implentation of expected-SARSA on the non-slippery FrozenLake environment with $\\gamma=1.0$.  Find the best $\\alpha$ and exploration strategy.  Plot the episode returns and episode lengths over number of training episodes.\n",
    "\n",
    "It might be helpful to also smooth the curves to get a clearer picture of the learning process.  If you do so, plot both the unsmoothened data and the smoothened data in the same figure, and remember to label the axes and the lines.  Make your plots readable **without having to look at your code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint:  you may temporarily change these values to help development, but make sure you set them back before submitting!\n",
    "num_runs, num_episodes, num_steps_max = 10, 1_000, 100\n",
    "returns_esarsa_fl = np.empty((num_runs, num_episodes))\n",
    "lengths_esarsa_fl = np.empty((num_runs, num_episodes), dtype=np.int)\n",
    "\n",
    "for i in tqdm.tnrange(num_runs, desc='Runs'):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Test your implentation of expected-SARSA on the slippery FrozenLake environment with $\\gamma=1.0$.  Find the best $\\alpha$ and exploration strategy.  Plot the episode returns and episode lengths over number of training episodes.\n",
    "\n",
    "It might be helpful to also smooth the curves to get a clearer picture of the learning process.  If you do so, plot both the unsmoothened data and the smoothened data in the same figure, and remember to label the axes and the lines.  Make your plots readable **without having to look at your code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint:  you may temporarily change these values to help development, but make sure you set them back before submitting!\n",
    "num_runs, num_episodes, num_steps_max = 10, 20_000, 100\n",
    "returns_esarsa_fls = np.empty((num_runs, num_episodes))\n",
    "lengths_esarsa_fls = np.empty((num_runs, num_episodes), dtype=np.int)\n",
    "\n",
    "for i in tqdm.tnrange(num_runs, desc='Runs'):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Test your implentation of expected-SARSA on the discrete CartPole environment with $\\gamma=1.0$.  Find the best $\\alpha$ and exploration strategy.  Plot the episode returns and episode lengths over number of training episodes.\n",
    "\n",
    "It might be helpful to also smooth the curves to get a clearer picture of the learning process.  If you do so, plot both the unsmoothened data and the smoothened data in the same figure, and remember to label the axes and the lines.  Make your plots readable **without having to look at your code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint:  you may temporarily change these values to help development, but make sure you set them back before submitting!\n",
    "num_runs, num_episodes, num_steps_max = 10, 30_000, 100\n",
    "returns_esarsa_cpd = np.empty((num_runs, num_episodes))\n",
    "lengths_esarsa_cpd = np.empty((num_runs, num_episodes), dtype=np.int)\n",
    "\n",
    "for i in tqdm.tnrange(num_runs, desc='Runs'):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (20 Points):\n",
    "\n",
    "#### a) Implement the off-policy TD control algorithm known as Q-learning. Make sure that the exploration schedule is applied based on how many time-steps have passed from the beginning of the training, not the beginning of the episode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, num_steps_max, *, alpha, gamma, exploration):\n",
    "    \"\"\"Q-learning algorithm.\n",
    "\n",
    "    Args:\n",
    "        - env: The Gym environment\n",
    "        - num_episodes: The number of episodes for which to train the agent\n",
    "        - num_steps_max: The maximum number of steps for a single episode\n",
    "        - alpha: The stepsize\n",
    "        - gamma: The discount factor\n",
    "        - exploration: epsilon schedule\n",
    "\n",
    "    Returns: (Q, returns, lengths)\n",
    "        - Q: Dictonary mapping state -> action values\n",
    "        - returns: Numpy array containing the return of each episode during training\n",
    "        - lengths: Numpy array containing the length of each episode during training\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    returns = np.empty(num_episodes)\n",
    "    lengths = np.empty(num_episodes, dtype=np.int)\n",
    "\n",
    "    for i in tqdm.tnrange(num_episodes, desc='Episodes', leave=False):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    return Q, returns, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Test your implentation of Q-learning on the non-slippery FrozenLake environment with $\\gamma=1.0$.  Find the best $\\alpha$ and exploration strategy.  Plot the episode returns and episode lengths over number of training episodes.\n",
    "\n",
    "It might be helpful to also smooth the curves to get a clearer picture of the learning process.  If you do so, plot both the unsmoothened data and the smoothened data in the same figure, and remember to label the axes and the lines.  Make your plots readable **without having to look at your code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint:  you may temporarily change these values to help development, but make sure you set them back before submitting!\n",
    "num_runs, num_episodes, num_steps_max = 50, 1_000, 100\n",
    "returns_ql_fl = np.empty((num_runs, num_episodes))\n",
    "lengths_ql_fl = np.empty((num_runs, num_episodes), dtype=np.int)\n",
    "\n",
    "for i in tqdm.tnrange(num_runs, desc='Runs'):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Test your implentation of Q-learning on the slippery FrozenLake environment with $\\gamma=1.0$.  Find the best $\\alpha$ and exploration strategy.  Plot the episode returns and episode lengths over number of training episodes.\n",
    "\n",
    "It might be helpful to also smooth the curves to get a clearer picture of the learning process.  If you do so, plot both the unsmoothened data and the smoothened data in the same figure, and remember to label the axes and the lines.  Make your plots readable **without having to look at your code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint:  you may temporarily change these values to help development, but make sure you set them back before submitting!\n",
    "num_runs, num_episodes, num_steps_max = 10, 20_000, 100\n",
    "returns_ql_fls = np.empty((num_runs, num_episodes))\n",
    "lengths_ql_fls = np.empty((num_runs, num_episodes), dtype=np.int)\n",
    "\n",
    "for i in tqdm.tnrange(num_runs, desc='Runs'):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Test your implentation of Q-learning on the discrete CartPole environment with $\\gamma=1.0$.  Find the best $\\alpha$ and exploration strategy.  Plot the episode returns and episode lengths over number of training episodes.\n",
    "\n",
    "It might be helpful to also smooth the curves to get a clearer picture of the learning process.  If you do so, plot both the unsmoothened data and the smoothened data in the same figure, and remember to label the axes and the lines.  Make your plots readable **without having to look at your code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint:  you may temporarily change these values to help development, but make sure you set them back before submitting!\n",
    "num_runs, num_episodes, num_steps_max = 10, 30_000, 100\n",
    "returns_ql_cpd = np.empty((num_runs, num_episodes))\n",
    "lengths_ql_cpd = np.empty((num_runs, num_episodes), dtype=np.int)\n",
    "\n",
    "for i in tqdm.tnrange(num_runs, desc='Runs'):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
